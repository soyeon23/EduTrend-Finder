import pandas as pd
from pytrends.request import TrendReq
import time
import random
from keyword_list import get_category
from concurrent.futures import ThreadPoolExecutor, as_completed

def create_pytrends():
    """ìƒˆë¡œìš´ PyTrends ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìŠ¤ë ˆë“œ ì•ˆì „)"""
    try:
        return TrendReq(hl='ko-KR', tz=540, timeout=(10, 25), retries=2, backoff_factor=0.1)
    except Exception:
        return None

# ê¸°ë³¸ PyTrends ì¸ìŠ¤í„´ìŠ¤ (ì—°ê´€ ê²€ìƒ‰ì–´ ë“± ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‘ì—…ìš©)
pytrends = create_pytrends()

def fetch_trend_data(keywords, timeframe='today 3-m', pytrends_instance=None):
    """
    Google Trends ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    MVPì—ì„œëŠ” 5ê°œì”© ë‚˜ëˆ„ì–´ ìš”ì²­ë¥¼ ë³´ëƒ…ë‹ˆë‹¤ (Rate Limit ë°©ì§€).
    ì‹¤íŒ¨ ì‹œ Mock Dataë¥¼ ë°˜í™˜í•  ìˆ˜ë„ ìˆë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    pt = pytrends_instance if pytrends_instance else pytrends
    if pt is None:
        pt = create_pytrends()
    if pt is None:
        return pd.DataFrame()

    all_data = pd.DataFrame()

    # ìµœëŒ€ 5ê°œì”© ì²­í¬ ë¶„í• 
    chunk_size = 5
    keyword_chunks = [keywords[i:i + chunk_size] for i in range(0, len(keywords), chunk_size)]

    for chunk in keyword_chunks:
        try:
            # ìµœì†Œ ë”œë ˆì´ (Rate Limit ë°©ì§€)
            time.sleep(random.uniform(0.3, 0.6))
            pt.build_payload(chunk, cat=0, timeframe=timeframe, geo='KR')
            data = pt.interest_over_time()
            
            if not data.empty:
                # isPartial ì»¬ëŸ¼ ì œê±°
                if 'isPartial' in data.columns:
                    data = data.drop(columns=['isPartial'])

                # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
                data = data.loc[:, ~data.columns.duplicated()]

                # ë°ì´í„° ë³‘í•©
                if all_data.empty:
                    all_data = data
                else:
                    # ê¸°ì¡´ ì»¬ëŸ¼ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ì¶”ê°€
                    new_cols = [c for c in data.columns if c not in all_data.columns]
                    if new_cols:
                        all_data = pd.concat([all_data, data[new_cols]], axis=1)
        except Exception as e:
            print(f"Error fetching chunk {chunk}: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ ì²­í¬ëŠ” ê±´ë„ˆë›°ê±°ë‚˜ 0ìœ¼ë¡œ ì±„ì›€
            pass
            
    return all_data

def fetch_related_queries(keyword, timeframe='today 3-m'):
    """
    íŠ¹ì • í‚¤ì›Œë“œì˜ ì—°ê´€ ê²€ìƒ‰ì–´(Related Queries)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        # ìµœì†Œ ë”œë ˆì´
        time.sleep(random.uniform(0.2, 0.4))
        pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='KR')
        related = pytrends.related_queries()
        
        if related and keyword in related:
            # top / rising ì¤‘ rising(ê¸‰ìƒìŠ¹) ìš°ì„ , ì—†ìœ¼ë©´ top
            rising_df = related[keyword]['rising']
            top_df = related[keyword]['top']
            
            queries = []
            if rising_df is not None and not rising_df.empty:
                queries.extend(rising_df['query'].head(5).tolist())
            
            if top_df is not None and not top_df.empty:
                queries.extend(top_df['query'].head(5).tolist())
                
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œ
            return list(dict.fromkeys(queries))[:10]
            
    except Exception as e:
        print(f"Error fetching related queries for {keyword}: {e}")
        pass
        
    return get_mock_related_queries(keyword)

def get_mock_related_queries(keyword):
    """
    ë°ëª¨ìš© ì—°ê´€ ê²€ìƒ‰ì–´ ë°˜í™˜
    """
    suffixes = ["ê°•ì˜", "ì…ë¬¸", "ìê²©ì¦", "ì±…", "ë¬´ë£Œ", "ì‚¬ìš©ë²•", "íŠœí† ë¦¬ì–¼", "ì „ë§", "ì·¨ì—…"]
    return [f"{keyword} {s}" for s in suffixes]

def calculate_growth_metrics(df):
    """
    ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ ì„±ì¥ë¥ , ìƒíƒœ, ê·¸ë¦¬ê³  'ì¶”ì„¸ ì§„ë‹¨(Diagnosis)'ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    results = []
    
    if df.empty:
        return pd.DataFrame()

    n_rows = len(df)
    if n_rows < 10:
        early_period = df
        recent_period = df
    else:
        split_idx = int(n_rows * 0.3)
        early_period = df.iloc[:split_idx]
        recent_period = df.iloc[-split_idx:]
    
    # 4ì£¼(ì•½ 28ì¼) ê¸°ì¤€ ìœˆë„ìš° ë¹„êµë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ê³„ì‚° (ë°ì´í„°ê°€ ì¼ë³„ì´ë¼ê³  ê°€ì •)
    last_4w_idx = max(0, n_rows - 28)
    prev_4w_idx = max(0, n_rows - 56)
        
    for keyword in df.columns:
        # 1. ê¸°ë³¸ í†µê³„
        series = df[keyword]
        # ì¤‘ë³µ ì»¬ëŸ¼ìœ¼ë¡œ ì¸í•´ DataFrameì´ ë°˜í™˜ë˜ëŠ” ê²½ìš° ì²˜ë¦¬
        if isinstance(series, pd.DataFrame):
            series = series.iloc[:, 0]

        early_series = early_period[keyword]
        recent_series = recent_period[keyword]
        if isinstance(early_series, pd.DataFrame):
            early_series = early_series.iloc[:, 0]
        if isinstance(recent_series, pd.DataFrame):
            recent_series = recent_series.iloc[:, 0]

        early_mean = float(early_series.mean())
        recent_mean = float(recent_series.mean())
        total_mean = float(series.mean())
        std_dev = float(series.std())
        max_val = float(series.max())

        # NaN ì²˜ë¦¬
        if pd.isna(early_mean):
            early_mean = 0.0
        if pd.isna(recent_mean):
            recent_mean = 0.0
        if pd.isna(total_mean):
            total_mean = 0.0
        if pd.isna(std_dev):
            std_dev = 0.0

        # 2. ì„±ì¥ë¥  ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€ + ë³´ì™„)
        if early_mean < 1.0:
            if recent_mean > 5.0:
                growth_rate = 999.0 # Low base effect
            else:
                growth_rate = 0.0
        else:
            growth_rate = ((recent_mean - early_mean) / early_mean) * 100
            
        # 3. ìƒì„¸ ì§„ë‹¨ ì§€í‘œ ê³„ì‚°
        
        # A) ë°ì´í„° ë¶€ì¡± ì—¬ë¶€ (0ì´ 30% ì´ìƒì´ë©´ ë¶€ì¡±ìœ¼ë¡œ ê°„ì£¼)
        zero_ratio = (series == 0).sum() / n_rows
        is_insufficient = zero_ratio > 0.3
        
        # B) ë³€ë™ì„± (ë³€ë™ê³„ìˆ˜ CV = std / mean)
        cv = std_dev / total_mean if total_mean > 0 else 0
        volatility_label = "ë†’ìŒ" if cv > 0.5 else ("ë³´í†µ" if cv > 0.2 else "ë‚®ìŒ")
        
        # C) ìŠ¤íŒŒì´í¬ ì—¬ë¶€ (ìµœëŒ€ê°’ì´ í‰ê· +2std ë³´ë‹¤ í¬ê±°ë‚˜, ìµœê·¼ í‰ê· ì˜ 2.5ë°° ì´ìƒ)
        is_spike = (max_val > total_mean + 2 * std_dev) or (max_val > 2.5 * recent_mean)
        
        # D) ìµœê·¼ ì¶”ì„¸ (ìµœê·¼ 4ì£¼ vs ì§ì „ 4ì£¼)
        recent_4w_mean = series.iloc[last_4w_idx:].mean()
        prev_4w_mean = series.iloc[prev_4w_idx:last_4w_idx].mean() if prev_4w_idx < last_4w_idx else early_mean
        
        is_rising_short_term = recent_4w_mean > prev_4w_mean * 1.1 # 10% ì´ìƒ ìƒìŠ¹
        
        # 4. ìµœì¢… ì§„ë‹¨ ë¼ë²¨ë§ (Heuristics)
        diagnosis_type = "ê´€ì°° í•„ìš”"
        diagnosis_reason = "íŠ¹ì´ ì‚¬í•­ ì—†ìŒ"
        caution_label = "-"
        
        if is_insufficient:
            diagnosis_type = "â›” ë°ì´í„° ë¶€ì¡±"
            diagnosis_reason = "ê²€ìƒ‰ ë°ì´í„°ì˜ 30% ì´ìƒì´ 0ìœ¼ë¡œ ì§‘ê³„ë¨"
            caution_label = "ë°ì´í„° ë¶€ì¡±"
        elif early_mean < 5 and growth_rate > 100:
            diagnosis_type = "âš ï¸ ì €ê´€ì‹¬ë„ ê¸‰ë“±"
            diagnosis_reason = "ì´ˆê¸° ê´€ì‹¬ë„ê°€ ë‚®ì•„ ì„±ì¥ë¥ ì´ ê³¼ì¥ë  ìˆ˜ ìˆìŒ (Base Effect)"
            caution_label = "ì €ê´€ì‹¬ë„ ê¸°ë°˜"
        elif is_spike and volatility_label == "ë†’ìŒ":
            diagnosis_type = "âš ï¸ ì¼ì‹œ ê¸‰ë“± (ì´ìŠˆì„±)"
            diagnosis_reason = "í‰ê·  ëŒ€ë¹„ ê³¼ë„í•œ ìŠ¤íŒŒì´í¬ ë°œìƒ, ë³€ë™ì„± í¼"
            caution_label = "ì¼ì‹œ ê¸‰ë“± ê°€ëŠ¥"
        elif growth_rate > 10 and is_rising_short_term and volatility_label != "ë†’ìŒ":
            diagnosis_type = "âœ… ì§€ì† ìƒìŠ¹"
            diagnosis_reason = "ìµœê·¼ êµ¬ê°„ í‰ê·  ìƒìŠ¹ì„¸ê°€ ëšœë ·í•˜ë©° ë³€ë™ì„± ì•ˆì •ì "
            caution_label = "ì§€ì† ìƒìŠ¹ ìœ ë ¥"
        elif growth_rate < -5:
            diagnosis_type = "ğŸ“‰ í•˜ë½ì„¸"
            diagnosis_reason = "ìµœê·¼ ê´€ì‹¬ë„ê°€ ì´ˆê¸° ëŒ€ë¹„ ê°ì†Œí•¨"
            caution_label = "í•˜ë½ ë°˜ì „"
        elif growth_rate > 0:
            diagnosis_type = "ğŸ“ˆ ì™„ë§Œí•œ ìƒìŠ¹"
            diagnosis_reason = "ê¸‰ê²©í•˜ì§€ ì•Šìœ¼ë‚˜ ìƒìŠ¹ íë¦„ ìœ ì§€ ì¤‘"
            caution_label = "ìƒìŠ¹ íë¦„"
        else:
            diagnosis_type = "â– ì •ì²´/ì•ˆì •"
            diagnosis_reason = "í° ë³€ë™ ì—†ì´ ê´€ì‹¬ë„ ìœ ì§€ ì¤‘"
            caution_label = "ì•ˆì •"

        # 5. Planning Insight & Action Recommendation (PM Logic)
        insight_target = "ì…ë¬¸/ê¸°ì´ˆ íƒ€ê²Ÿ ì í•©" if "ê¸°ì´ˆ" in keyword or "ì…ë¬¸" in keyword else "ì‹¤ë¬´/ì¤‘ê¸‰ íƒ€ê²Ÿ ê³ ë ¤"
        if "ìê²©ì¦" in keyword: insight_target = "ì·¨ì—…/ìê²©ì¦ ì·¨ë“ ëª©í‘œ íƒ€ê²Ÿ"
        
        insight_position = "íŠ¸ë Œë“œ ë¦¬í¬íŠ¸í˜• ì½˜í…ì¸ " if volatility_label == "ë†’ìŒ" else "ì»¤ë¦¬í˜ëŸ¼í˜•/ë¡œë“œë§µ ì½˜í…ì¸ "
        
        # Action Logic
        action_label = "ì´ë²ˆ ë¶„ê¸° ê¸°íš ì œì™¸" # Default
        if diagnosis_type == "âœ… ì§€ì† ìƒìŠ¹":
            action_label = "ğŸš€ ì‹ ê·œ ê¸°íš ê²€í† "
            insight_risk = "ìˆ˜ìš” ê²€ì¦ë¨. ì°¨ë³„í™”ëœ ì‹¬í™” ì£¼ì œ ë°œêµ´ í•„ìš”."
        elif diagnosis_type == "â– ì •ì²´/ì•ˆì •":
            if recent_mean > 40:
                action_label = "ğŸ”„ ê¸°ì¡´ ê³¼ì • ë¦¬ë‰´ì–¼"
                insight_risk = "ìˆ˜ìš” ì•ˆì •ì . ìµœì‹  íŠ¸ë Œë“œ ë°˜ì˜í•œ ë¦¬ë‰´ì–¼ ê¶Œì¥."
            else:
                action_label = "â– í˜„ìƒ ìœ ì§€/ë³´ë¥˜"
                insight_risk = "í° ë³€ë™ ì—†ìŒ. ë¦¬ì†ŒìŠ¤ íˆ¬ì… ëŒ€ë¹„ ì„±ê³¼ ë‚®ì„ ìˆ˜ ìˆìŒ."
        elif "ê¸‰ë“±" in diagnosis_type or volatility_label == "ë†’ìŒ":
            action_label = "ğŸ§ª ë‹¨ê¸° í…ŒìŠ¤íŠ¸ ì½˜í…ì¸ "
            insight_risk = "ì¼ì‹œì  ìœ í–‰ ê°€ëŠ¥ì„±. ì›¨ë¹„ë‚˜/íŠ¹ê°•ìœ¼ë¡œ ê°€ë³ê²Œ ìˆ˜ìš” ê²€ì¦."
        elif diagnosis_type == "ğŸ“‰ í•˜ë½ì„¸":
            action_label = "â›” ì´ë²ˆ ë¶„ê¸° ê¸°íš ì œì™¸"
            insight_risk = "ê´€ì‹¬ë„ í•˜ë½ ì¤‘. ì‹ ê·œ ì§„ì… ì‹œ ë¦¬ìŠ¤í¬ í¼."
        elif is_insufficient:
             action_label = "â›” ë°ì´í„° ë¶€ì¡±"
             insight_risk = "íŒë‹¨ ê·¼ê±° ë¶€ì¡±."

        results.append({
            'í‚¤ì›Œë“œ': keyword,
            'ì„±ì¥ë¥ (%)': round(growth_rate, 1),
            'ìµœê·¼ ê´€ì‹¬ë„': round(recent_mean, 1),
            'ìƒíƒœ': diagnosis_type.split(" ")[-1] if " " in diagnosis_type else diagnosis_type, 
            'ì§„ë‹¨ìœ í˜•': diagnosis_type,
            'ì§„ë‹¨ê·¼ê±°': diagnosis_reason,
            'ì£¼ì˜ë¼ë²¨': caution_label,
            'ë³€ë™ì„±': volatility_label,
            'ì¶”ì²œì•¡ì…˜': action_label, # New Column
            'ê¸°íš_íƒ€ê²Ÿ': insight_target,
            'ê¸°íš_í¬ì§€ì…˜': insight_position,
            'ê¸°íš_ë¦¬ìŠ¤í¬': insight_risk,
            'ì¹´í…Œê³ ë¦¬': get_category(keyword)
        })
        
    return pd.DataFrame(results)

def get_mock_data(keywords, timeframe='today 3-m'):
    """
    PyTrends ì—°ê²° ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ë°ëª¨ìš© Mock Data ìƒì„±
    """
    dates = pd.date_range(end=pd.Timestamp.now(), periods=90) # approx 3 months
    df = pd.DataFrame(index=dates)

    for kw in keywords:
        # ëœë¤ ì¶”ì„¸ ìƒì„±
        base = random.randint(10, 50)
        trend = random.choice([-0.1, 0, 0.2, 0.5]) # í•˜ë½, ì •ì²´, ìƒìŠ¹, ê¸‰ìƒìŠ¹
        noise = [random.randint(-5, 5) for _ in range(90)]
        values = [max(0, min(100, base + (i * trend) + n)) for i, n in enumerate(noise)]
        df[kw] = values

    return df


# =============================================================================
# ë‹¤ì¤‘ ì‹ í˜¸ ë°ì´í„° ìˆ˜ì§‘ (YouTube Search ì¶”ê°€)
# =============================================================================

def fetch_youtube_trend_data(keywords, timeframe='today 3-m', pytrends_instance=None):
    """
    YouTube Search íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    gprop='youtube'ë¡œ YouTube ê²€ìƒ‰ íŠ¸ë Œë“œ ìˆ˜ì§‘.
    """
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    pt = pytrends_instance if pytrends_instance else pytrends
    if pt is None:
        pt = create_pytrends()
    if pt is None:
        return pd.DataFrame()

    all_data = pd.DataFrame()
    chunk_size = 5
    keyword_chunks = [keywords[i:i + chunk_size] for i in range(0, len(keywords), chunk_size)]

    for chunk in keyword_chunks:
        try:
            # ìµœì†Œ ë”œë ˆì´ (Rate Limit ë°©ì§€)
            time.sleep(random.uniform(0.3, 0.6))
            pt.build_payload(chunk, cat=0, timeframe=timeframe, geo='KR', gprop='youtube')
            data = pt.interest_over_time()

            if not data.empty:
                if 'isPartial' in data.columns:
                    data = data.drop(columns=['isPartial'])

                # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
                data = data.loc[:, ~data.columns.duplicated()]

                if all_data.empty:
                    all_data = data
                else:
                    # ê¸°ì¡´ ì»¬ëŸ¼ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ì¶”ê°€
                    new_cols = [c for c in data.columns if c not in all_data.columns]
                    if new_cols:
                        all_data = pd.concat([all_data, data[new_cols]], axis=1)
        except Exception as e:
            print(f"Error fetching YouTube chunk {chunk}: {e}")
            pass

    return all_data


def fetch_multi_signal_data(keywords, timeframe='today 3-m'):
    """
    ì›¹ ê²€ìƒ‰ê³¼ YouTube ê²€ìƒ‰ íŠ¸ë Œë“œë¥¼ ë³‘ë ¬ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ê° ìŠ¤ë ˆë“œì—ì„œ ë³„ë„ì˜ PyTrends ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½í•© ë°©ì§€.
    Returns: dict with 'web' and 'youtube' DataFrames
    """
    results = {'web': pd.DataFrame(), 'youtube': pd.DataFrame()}

    def fetch_web():
        # ê° ìŠ¤ë ˆë“œì—ì„œ ë…ë¦½ì ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        pt = create_pytrends()
        return fetch_trend_data(keywords, timeframe, pytrends_instance=pt)

    def fetch_youtube():
        # ê° ìŠ¤ë ˆë“œì—ì„œ ë…ë¦½ì ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        pt = create_pytrends()
        return fetch_youtube_trend_data(keywords, timeframe, pytrends_instance=pt)

    # ë³‘ë ¬ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=2) as executor:
        web_future = executor.submit(fetch_web)
        youtube_future = executor.submit(fetch_youtube)

        results['web'] = web_future.result()
        results['youtube'] = youtube_future.result()

    return results


def get_mock_youtube_data(keywords):
    """
    YouTube íŠ¸ë Œë“œ ë°ëª¨ìš© Mock Data ìƒì„±.
    ì›¹ ê²€ìƒ‰ê³¼ ë‹¤ë¥¸ íŒ¨í„´ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    """
    dates = pd.date_range(end=pd.Timestamp.now(), periods=90)
    df = pd.DataFrame(index=dates)

    for kw in keywords:
        # YouTubeëŠ” ì›¹ê³¼ ë‹¤ë¥¸ íŒ¨í„´ (ì˜ìƒ ì½˜í…ì¸  íŠ¹ì„± ë°˜ì˜)
        base = random.randint(5, 40)
        # YouTubeëŠ” ê¸‰ë“±/ê¸‰ë½ì´ ë” ë¹ˆë²ˆí•œ ê²½í–¥
        trend = random.choice([-0.2, 0, 0.3, 0.8])
        noise = [random.randint(-8, 8) for _ in range(90)]
        values = [max(0, min(100, base + (i * trend) + n)) for i, n in enumerate(noise)]
        df[kw] = values

    return df


def analyze_cross_signals(web_metrics, youtube_df, keywords):
    """
    ì›¹ ê²€ìƒ‰ê³¼ YouTube ê²€ìƒ‰ ì‹ í˜¸ë¥¼ êµì°¨ ë¶„ì„í•˜ì—¬
    'í•™ìŠµ ì˜ë„ ì‹ í˜¸ ê°•ë„'ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.

    ëª©ì : ì •í™•í•œ ì˜ˆì¸¡ì´ ì•„ë‹Œ, íŒë‹¨ ë³´ì™„ì„ ìœ„í•œ ì‹ í˜¸ êµì°¨ í™•ì¸

    Returns: DataFrame with cross-signal analysis
    """
    results = []

    if youtube_df.empty:
        youtube_df = get_mock_youtube_data(keywords)

    # YouTube ì„±ì¥ë¥  ê³„ì‚°
    n_rows = len(youtube_df) if not youtube_df.empty else 0

    for kw in keywords:
        web_row = web_metrics[web_metrics['í‚¤ì›Œë“œ'] == kw]
        if web_row.empty:
            continue

        web_growth = web_row.iloc[0]['ì„±ì¥ë¥ (%)']
        web_recent = web_row.iloc[0]['ìµœê·¼ ê´€ì‹¬ë„']
        web_volatility = web_row.iloc[0]['ë³€ë™ì„±']

        # YouTube ì§€í‘œ ê³„ì‚°
        if kw in youtube_df.columns and n_rows >= 10:
            yt_series = youtube_df[kw]
            # ì¤‘ë³µ ì»¬ëŸ¼ìœ¼ë¡œ ì¸í•´ DataFrameì´ ë°˜í™˜ë˜ëŠ” ê²½ìš° ì²˜ë¦¬
            if isinstance(yt_series, pd.DataFrame):
                yt_series = yt_series.iloc[:, 0]

            split_idx = int(n_rows * 0.3)
            yt_early = float(yt_series.iloc[:split_idx].mean())
            yt_recent = float(yt_series.iloc[-split_idx:].mean())

            # NaN ì²´í¬
            if pd.isna(yt_early) or yt_early < 1.0:
                yt_growth = 999.0 if (not pd.isna(yt_recent) and yt_recent > 5.0) else 0.0
            else:
                yt_growth = ((yt_recent - yt_early) / yt_early) * 100
        else:
            # Mock ë˜ëŠ” ë°ì´í„° ì—†ìŒ
            yt_growth = web_growth * random.uniform(0.5, 1.5)
            yt_recent = web_recent * random.uniform(0.3, 1.2)

        # === êµì°¨ ì‹ í˜¸ í•´ì„ ===
        signal_pattern = ""
        signal_interpretation = ""
        signal_strength = "ë³´í†µ"  # ë‚®ìŒ, ë³´í†µ, ë†’ìŒ

        # íŒ¨í„´ 1: ì›¹ ì•ˆì • + YouTube ê¸‰ìƒìŠ¹ â†’ ì˜ìƒ í•™ìŠµ ìˆ˜ìš” ì¦ê°€ ì‹ í˜¸
        if abs(web_growth) < 15 and yt_growth > 30:
            signal_pattern = "ì˜ìƒ ì „í™˜ ì‹ í˜¸"
            signal_interpretation = "ì›¹ ê²€ìƒ‰ì€ ì•ˆì •ì ì´ë‚˜ YouTube ê²€ìƒ‰ ê¸‰ìƒìŠ¹. ì˜ìƒ/ê°•ì˜ í˜•íƒœ ìˆ˜ìš” ì¦ê°€ ê°€ëŠ¥ì„±."
            signal_strength = "ë†’ìŒ"

        # íŒ¨í„´ 2: ì›¹ + YouTube ëª¨ë‘ ìƒìŠ¹ â†’ ê°•í•œ í•™ìŠµ ìˆ˜ìš” ì‹ í˜¸
        elif web_growth > 10 and yt_growth > 10:
            signal_pattern = "ë³µí•© ìƒìŠ¹ ì‹ í˜¸"
            signal_interpretation = "ì›¹ê³¼ YouTube ëª¨ë‘ ìƒìŠ¹ì„¸. ë‹¤ì–‘í•œ í˜•íƒœì˜ í•™ìŠµ íƒìƒ‰ì´ í™œë°œí•¨."
            signal_strength = "ë†’ìŒ"

        # íŒ¨í„´ 3: ì›¹ ìƒìŠ¹ + YouTube ì •ì²´/í•˜ë½ â†’ ì •ë³´ íƒìƒ‰ ë‹¨ê³„
        elif web_growth > 15 and yt_growth < 5:
            signal_pattern = "ì •ë³´ íƒìƒ‰ ë‹¨ê³„"
            signal_interpretation = "ì›¹ ê²€ìƒ‰ ì¦ê°€ ì¤‘ì´ë‚˜ ì˜ìƒ íƒìƒ‰ì€ ë¯¸ë¯¸. ì•„ì§ í•™ìŠµ ë‹¨ê³„ ì§„ì… ì „ì¼ ìˆ˜ ìˆìŒ."
            signal_strength = "ë³´í†µ"

        # íŒ¨í„´ 4: ì›¹ í•˜ë½ + YouTube ìƒìŠ¹ â†’ í•™ìŠµ ì±„ë„ ì´ë™
        elif web_growth < -5 and yt_growth > 10:
            signal_pattern = "ì±„ë„ ì´ë™ ì‹ í˜¸"
            signal_interpretation = "ì›¹ ê²€ìƒ‰ ê°ì†Œ, YouTube ê²€ìƒ‰ ì¦ê°€. í•™ìŠµ ì±„ë„ì´ ì˜ìƒìœ¼ë¡œ ì´ë™ ì¤‘ì¼ ìˆ˜ ìˆìŒ."
            signal_strength = "ë³´í†µ"

        # íŒ¨í„´ 5: ë‘˜ ë‹¤ í•˜ë½ â†’ ê´€ì‹¬ ê°ì†Œ
        elif web_growth < -5 and yt_growth < -5:
            signal_pattern = "ê´€ì‹¬ ê°ì†Œ ì‹ í˜¸"
            signal_interpretation = "ì›¹ê³¼ YouTube ëª¨ë‘ í•˜ë½ì„¸. ì „ë°˜ì ì¸ ê´€ì‹¬ ê°ì†Œ ì¶”ì„¸."
            signal_strength = "ë‚®ìŒ"

        # íŒ¨í„´ 6: ë‘˜ ë‹¤ ì •ì²´ â†’ ì•ˆì •ì  ìˆ˜ìš”
        elif abs(web_growth) < 10 and abs(yt_growth) < 10:
            signal_pattern = "ì•ˆì • ìœ ì§€"
            signal_interpretation = "ì›¹ê³¼ YouTube ëª¨ë‘ í° ë³€ë™ ì—†ìŒ. ê¸°ì¡´ ê´€ì‹¬ë„ ìœ ì§€ ì¤‘."
            signal_strength = "ë³´í†µ"

        else:
            signal_pattern = "í˜¼í•© ì‹ í˜¸"
            signal_interpretation = "ëª…í™•í•œ íŒ¨í„´ ì—†ìŒ. ì¶”ê°€ ê´€ì°° í•„ìš”."
            signal_strength = "ë‚®ìŒ"

        results.append({
            'í‚¤ì›Œë“œ': kw,
            'ì›¹_ì„±ì¥ë¥ ': round(web_growth, 1),
            'YouTube_ì„±ì¥ë¥ ': round(yt_growth, 1),
            'ì›¹_ê´€ì‹¬ë„': round(web_recent, 1),
            'ì‹ í˜¸_íŒ¨í„´': signal_pattern,
            'ì‹ í˜¸_í•´ì„': signal_interpretation,
            'ì‹ í˜¸_ê°•ë„': signal_strength,
            'ì›¹_ë³€ë™ì„±': web_volatility
        })

    return pd.DataFrame(results)


# =============================================================================
# ë°ì´í„° í•œê³„ ëª…ì‹œ í…ìŠ¤íŠ¸ (UXìš©)
# =============================================================================

DATA_LIMITATIONS = {
    'main_notice': """
        ì´ ë°ì´í„°ëŠ” 'ì •ë‹µ'ì´ ì•„ë‹Œ 'íŒë‹¨ì„ ë•ëŠ” ì‹ í˜¸(Signal)'ì…ë‹ˆë‹¤.
    """,

    'limitations': [
        {
            'title': 'ìƒëŒ€ ì§€ìˆ˜',
            'desc': 'Google TrendsëŠ” ì‹¤ì œ ê²€ìƒ‰ëŸ‰ì´ ì•„ë‹Œ ìƒëŒ€ì  ê´€ì‹¬ë„(0-100)ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ ê°„ ì ˆëŒ€ í¬ê¸° ë¹„êµëŠ” ì œí•œë©ë‹ˆë‹¤.'
        },
        {
            'title': 'í•™ìŠµ ì˜ë„ êµ¬ë¶„ í•œê³„',
            'desc': 'ê²€ìƒ‰ ì¦ê°€ê°€ ë°˜ë“œì‹œ êµìœ¡ ìˆ˜ìš” ì¦ê°€ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¼ë°˜ ê´€ì‹¬ê³¼ í•™ìŠµ ì˜ë„ëŠ” êµ¬ë¶„ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
        },
        {
            'title': 'ì§€ì†ì„± íŒë‹¨',
            'desc': 'ë‹¨ê¸° ì´ìŠˆì„± ê¸‰ë“±ê³¼ ì¤‘Â·ì¥ê¸° í•™ìŠµ ìˆ˜ìš”ë¥¼ ì™„ë²½íˆ êµ¬ë¶„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.'
        }
    ],

    'cross_signal_purpose': """
        YouTube ê²€ìƒ‰ ë°ì´í„°ëŠ” 'ì •í™•í•œ ìˆ˜ìš” ì˜ˆì¸¡'ì´ ì•„ë‹Œ,
        ì„œë¡œ ë‹¤ë¥¸ í”Œë«í¼ì—ì„œ ë™ì¼ í‚¤ì›Œë“œ ê´€ì‹¬ì´ ë°˜ë³µë˜ëŠ”ì§€ í™•ì¸í•˜ì—¬
        ë‹¨ì¼ ì§€í‘œì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ìš©ë„ì…ë‹ˆë‹¤.
    """,

    'positioning': "EduTrend Finder DataSource : Web Â· YouTube Â· Google Trends"
}
